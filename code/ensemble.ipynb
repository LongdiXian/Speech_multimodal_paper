{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3663acce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af757a35",
   "metadata": {},
   "source": [
    "# Single DLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef883ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, ComplementNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.special import expit\n",
    "import glob, os\n",
    "\n",
    "# ================= Calibration functions =================\n",
    "def platt_cv(p, y, n_splits=5):\n",
    "    out = np.zeros_like(p)\n",
    "    skf = StratifiedKFold(n_splits, shuffle=True, random_state=42)\n",
    "    for tr, te in skf.split(p, y):\n",
    "        lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        lr.fit(p[tr].reshape(-1,1), y[tr])\n",
    "        out[te] = lr.predict_proba(p[te].reshape(-1,1))[:,1]\n",
    "    return out\n",
    "\n",
    "def isotonic_cv(p, y, n_splits=5):\n",
    "    out = np.zeros_like(p)\n",
    "    skf = StratifiedKFold(n_splits, shuffle=True, random_state=42)\n",
    "    for tr, te in skf.split(p, y):\n",
    "        iso = IsotonicRegression(out_of_bounds='clip')\n",
    "        iso.fit(p[tr], y[tr])\n",
    "        out[te] = iso.transform(p[te])\n",
    "    return out\n",
    "\n",
    "# ================= Candidate stacking models =================\n",
    "candidate_models = {\n",
    "    'LR': LogisticRegression(solver='lbfgs', max_iter=2000),\n",
    "    'Ridge': RidgeClassifier(max_iter=2000),\n",
    "    'SGD': SGDClassifier(max_iter=2000, tol=1e-5),\n",
    "    'Perceptron': Perceptron(max_iter=2000),\n",
    "    'PassiveAggressive': PassiveAggressiveClassifier(max_iter=2000),\n",
    "    'RF': RandomForestClassifier(n_estimators=500, max_depth=5, random_state=6),\n",
    "    'ExtraTrees': ExtraTreesClassifier(n_estimators=500, max_depth=5, random_state=6),\n",
    "    'GBDT': GradientBoostingClassifier(n_estimators=500, learning_rate=1e-5),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=500, learning_rate=1e-5),\n",
    "    'Bagging': BaggingClassifier(n_estimators=500),\n",
    "    'XGB': XGBClassifier(n_estimators=500, learning_rate=1e-5,\n",
    "                          use_label_encoder=False, eval_metric='logloss'),\n",
    "    'SVC_rbf': SVC(probability=True, kernel='rbf'),\n",
    "    'SVC_linear': SVC(probability=True, kernel='linear'),\n",
    "    'NuSVC': NuSVC(probability=True),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'BernoulliNB': BernoulliNB(),\n",
    "    'ComplementNB': ComplementNB(),\n",
    "    'MLP_small': MLPClassifier(hidden_layer_sizes=(32,16), max_iter=2000, random_state=6),\n",
    "    'MLP_large': MLPClassifier(hidden_layer_sizes=(64,32), max_iter=2000, random_state=6)\n",
    "}\n",
    "\n",
    "# ================= Layer-wise Boosting models =================\n",
    "layer_boosters = {\n",
    "    'Layer_GBDT': GradientBoostingClassifier(\n",
    "        n_estimators=300, learning_rate=0.05, max_depth=3),\n",
    "    'Layer_XGB': XGBClassifier(\n",
    "        n_estimators=300, learning_rate=0.05, max_depth=3,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        eval_metric='logloss', use_label_encoder=False)\n",
    "}\n",
    "\n",
    "# ================= Multi-layer stacking + Boosting =================\n",
    "def multi_layer_stacking(df, prob_cols, label_col='label',\n",
    "                          max_layers=10, topN=5):\n",
    "\n",
    "    y = df[label_col].values\n",
    "    base_probs = [df[c].values for c in prob_cols]\n",
    "\n",
    "    # Initial calibration\n",
    "    probs_platt = [platt_cv(p, y) for p in base_probs]\n",
    "    probs_iso = [isotonic_cv(p, y) for p in base_probs]\n",
    "    current_probs = np.vstack(probs_platt + probs_iso).T\n",
    "\n",
    "    skf = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "    best_overall_auc = 0\n",
    "    best_overall_prob = None\n",
    "    layer = 1\n",
    "\n",
    "    while layer <= max_layers:\n",
    "        print(f\"\\n=== Layer {layer} ===\")\n",
    "        layer_probs = {}\n",
    "        layer_auc = {}\n",
    "\n",
    "        # ---------- normal stacking models ----------\n",
    "        for name, model in candidate_models.items():\n",
    "            stack_prob = np.zeros_like(y, dtype=float)\n",
    "\n",
    "            for tr, te in skf.split(current_probs, y):\n",
    "                model.fit(current_probs[tr], y[tr])\n",
    "                if hasattr(model, \"predict_proba\"):\n",
    "                    stack_prob[te] = model.predict_proba(current_probs[te])[:,1]\n",
    "                elif hasattr(model, \"decision_function\"):\n",
    "                    stack_prob[te] = expit(model.decision_function(current_probs[te]))\n",
    "                else:\n",
    "                    stack_prob[te] = model.predict(current_probs[te])\n",
    "\n",
    "            orig_prob = current_probs.mean(axis=1)\n",
    "            ensemble_prob = 0.5 * stack_prob + 0.5 * orig_prob\n",
    "\n",
    "            layer_probs[name] = ensemble_prob\n",
    "            layer_auc[name] = roc_auc_score(y, ensemble_prob)\n",
    "            print(f\"{name}: AUC={layer_auc[name]:.4f}\")\n",
    "\n",
    "        # ---------- layer-wise boosting ----------\n",
    "        for bname, booster in layer_boosters.items():\n",
    "            boost_prob = np.zeros_like(y, dtype=float)\n",
    "\n",
    "            for tr, te in skf.split(current_probs, y):\n",
    "                booster.fit(current_probs[tr], y[tr])\n",
    "                boost_prob[te] = booster.predict_proba(current_probs[te])[:,1]\n",
    "\n",
    "            orig_prob = current_probs.mean(axis=1)\n",
    "            ensemble_boost = 0.5 * boost_prob + 0.5 * orig_prob\n",
    "\n",
    "            layer_probs[bname] = ensemble_boost\n",
    "            layer_auc[bname] = roc_auc_score(y, ensemble_boost)\n",
    "            print(f\"{bname}: AUC={layer_auc[bname]:.4f}\")\n",
    "\n",
    "        # ---------- select best ----------\n",
    "        best_model = max(layer_auc, key=lambda k: layer_auc[k])\n",
    "        best_auc = layer_auc[best_model]\n",
    "        print(f\"✅ Layer {layer} best: {best_model} | AUC={best_auc:.4f}\")\n",
    "\n",
    "        if best_auc <= best_overall_auc:\n",
    "            print(\"❌ AUC not improved. Stop.\")\n",
    "            break\n",
    "\n",
    "        best_overall_auc = best_auc\n",
    "        best_overall_prob = layer_probs[best_model]\n",
    "        df[f'layer{layer}_best_{best_model}'] = best_overall_prob\n",
    "\n",
    "        # ---------- top-N enter next layer ----------\n",
    "        topN_models = sorted(layer_auc.items(),\n",
    "                             key=lambda x: x[1], reverse=True)[:topN]\n",
    "        topN_probs = np.vstack(\n",
    "            [layer_probs[name] for name, _ in topN_models]).T\n",
    "\n",
    "        topN_platt = [platt_cv(p, y) for p in topN_probs.T]\n",
    "        topN_iso = [isotonic_cv(p, y) for p in topN_probs.T]\n",
    "        current_probs = np.vstack(topN_platt + topN_iso).T\n",
    "\n",
    "        layer += 1\n",
    "\n",
    "    df['final_best_ensemble'] = best_overall_prob\n",
    "    return df, best_overall_auc\n",
    "\n",
    "# ================= Load datasets =================\n",
    "\n",
    "# data1\n",
    "data1 = pd.read_csv('OPENSMILE_probabilities.csv')\n",
    "prob_cols1 = [c for c in data1.columns if c not in ['case','label']]\n",
    "df1, auc1 = multi_layer_stacking(data1, prob_cols1)\n",
    "df1.to_csv('data1_multi_layer_ensemble_boosting.csv', index=False)\n",
    "\n",
    "# data2\n",
    "data2 = pd.read_csv('./result/Normal single models results/Normal_seven_results.csv')\n",
    "prob_cols2 = [c for c in data2.columns if c not in ['case','label']]\n",
    "df2, auc2 = multi_layer_stacking(data2, prob_cols2)\n",
    "df2.to_csv('data2_multi_layer_ensemble_boosting.csv', index=False)\n",
    "\n",
    "# data3 (multiple CSV)\n",
    "data3_folder = './result/text_model/'\n",
    "files = glob.glob(os.path.join(data3_folder, '*.csv'))\n",
    "\n",
    "dfs = []\n",
    "for i, f in enumerate(files):\n",
    "    tmp = pd.read_csv(f)\n",
    "    dfs.append(tmp[['prob']].rename(columns={'prob': f'prob{i}'}))\n",
    "\n",
    "labels = pd.read_csv(files[0])['label'].values\n",
    "df3 = pd.concat(dfs, axis=1)\n",
    "df3['label'] = labels\n",
    "\n",
    "df3, auc3 = multi_layer_stacking(df3, [c for c in df3.columns if c != 'label'])\n",
    "df3.to_csv('data3_multi_layer_ensemble_boosting.csv', index=False)\n",
    "\n",
    "# data4\n",
    "data4 = pd.read_csv('lingustic_probabilities.csv')\n",
    "prob_cols4 = [c for c in data4.columns if c not in ['case','label']]\n",
    "df4, auc4 = multi_layer_stacking(data4, prob_cols4)\n",
    "df4.to_csv('data4_multi_layer_ensemble_boosting.csv', index=False)\n",
    "\n",
    "print(f\"\\n✅ Final AUCs:\")\n",
    "print(f\"data1={auc1:.4f}, data2={auc2:.4f}, data3={auc3:.4f}, data4={auc4:.4f}\")\n",
    "print(\"✅ All boosting multi-layer ensemble results saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9d813",
   "metadata": {},
   "source": [
    "# multi DLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb45e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, ComplementNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.special import expit\n",
    "\n",
    "# ================= Load existing model probabilities =================\n",
    "data1 = pd.read_csv('OPENSMILE_probabilities.csv')\n",
    "data2 = pd.read_csv('./result/Normal single models results/Normal_seven_results.csv')\n",
    "data3 = pd.read_csv('./result/clinical_bert-base-cantonese_results.csv')\n",
    "data4 = pd.read_csv('lingustic_probabilities.csv')\n",
    "\n",
    "df = (\n",
    "    data1[['case', 'Decision Tree', 'label']].rename(columns={'Decision Tree': 'prob1'})\n",
    "    .merge(data2[['case', 'wave_prob']].rename(columns={'wave_prob': 'prob2'}), on='case')\n",
    "    .merge(data3[['case', 'prob1']].rename(columns={'prob1': 'prob3'}), on='case')\n",
    "    .merge(data4[['case', 'Extra Trees']].rename(columns={'Extra Trees': 'prob4'}), on='case')\n",
    ")\n",
    "\n",
    "y = df['label'].values\n",
    "\n",
    "# ================= Calibration Functions =================\n",
    "def platt_cv(p, y, n_splits=5):\n",
    "    out = np.zeros_like(p)\n",
    "    skf = StratifiedKFold(n_splits, shuffle=True, random_state=42)\n",
    "    for tr, te in skf.split(p, y):\n",
    "        lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        lr.fit(p[tr].reshape(-1,1), y[tr])\n",
    "        out[te] = lr.predict_proba(p[te].reshape(-1,1))[:,1]\n",
    "    return out\n",
    "\n",
    "def isotonic_cv(p, y, n_splits=5):\n",
    "    out = np.zeros_like(p)\n",
    "    skf = StratifiedKFold(n_splits, shuffle=True, random_state=42)\n",
    "    for tr, te in skf.split(p, y):\n",
    "        iso = IsotonicRegression(out_of_bounds='clip')\n",
    "        iso.fit(p[tr], y[tr])\n",
    "        out[te] = iso.transform(p[te])\n",
    "    return out\n",
    "\n",
    "# ================= Prepare model probabilities =================\n",
    "existing_probs = [df[f'prob{i}'].values for i in range(1,5)]\n",
    "probs_platt = [platt_cv(p, y) for p in existing_probs]\n",
    "probs_iso = [isotonic_cv(p, y) for p in existing_probs]\n",
    "all_probs = np.vstack(probs_platt + probs_iso).T\n",
    "\n",
    "# ================= Candidate stacking models =================\n",
    "candidate_models = {\n",
    "    'LR': LogisticRegression(solver='lbfgs', max_iter=2000),\n",
    "    'Ridge': RidgeClassifier(max_iter=2000),\n",
    "    'SGD': SGDClassifier(max_iter=2000, tol=1e-5),\n",
    "    'Perceptron': Perceptron(max_iter=2000),\n",
    "    'PassiveAggressive': PassiveAggressiveClassifier(max_iter=2000),\n",
    "    'RF': RandomForestClassifier(n_estimators=500, max_depth=5, random_state=6),\n",
    "    'ExtraTrees': ExtraTreesClassifier(n_estimators=500, max_depth=5, random_state=6),\n",
    "    'GBDT': GradientBoostingClassifier(n_estimators=500, learning_rate=1e-5),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=500, learning_rate=1e-5),\n",
    "    'Bagging': BaggingClassifier(n_estimators=500),\n",
    "    'XGB': XGBClassifier(n_estimators=500, learning_rate=1e-5, use_label_encoder=False, eval_metric='logloss'),\n",
    "    'SVC_rbf': SVC(probability=True, kernel='rbf'),\n",
    "    'SVC_linear': SVC(probability=True, kernel='linear'),\n",
    "    'NuSVC': NuSVC(probability=True),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'BernoulliNB': BernoulliNB(),\n",
    "    'ComplementNB': ComplementNB(),\n",
    "    'MLP_small': MLPClassifier(hidden_layer_sizes=(32,16), max_iter=2000, random_state=6),\n",
    "    'MLP_large': MLPClassifier(hidden_layer_sizes=(64,32), max_iter=2000, random_state=6)\n",
    "}\n",
    "\n",
    "# ================= Automated multi-layer stacking =================\n",
    "skf = StratifiedKFold(5, shuffle=True, random_state=6)\n",
    "current_probs = all_probs\n",
    "best_overall_auc = 0\n",
    "best_overall_prob = None\n",
    "layer = 1\n",
    "\n",
    "while True:\n",
    "    print(f\"\\n=== Layer {layer} ===\")\n",
    "    layer_probs = {}\n",
    "    layer_auc = {}\n",
    "    \n",
    "    for name, model in candidate_models.items():\n",
    "        stack_prob = np.zeros_like(y, dtype=float)\n",
    "        for tr, te in skf.split(current_probs, y):\n",
    "            model.fit(current_probs[tr], y[tr])\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                stack_prob[te] = model.predict_proba(current_probs[te])[:,1]\n",
    "            elif hasattr(model, \"decision_function\"):\n",
    "                stack_prob[te] = expit(model.decision_function(current_probs[te]))\n",
    "            else:\n",
    "                stack_prob[te] = model.predict(current_probs[te])\n",
    "        # 50% stacking + 50% 平均概率\n",
    "        orig_prob = current_probs[:, :min(current_probs.shape[1],5)].mean(axis=1)\n",
    "        ensemble_prob = 0.5 * stack_prob + 0.5 * orig_prob\n",
    "        layer_probs[name] = ensemble_prob\n",
    "        layer_auc[name] = roc_auc_score(y, ensemble_prob)\n",
    "        print(f\"{name}: AUC={layer_auc[name]:.4f}\")\n",
    "    \n",
    "    # 找出本层最佳\n",
    "    best_model = max(layer_auc, key=lambda k: layer_auc[k])\n",
    "    best_auc = layer_auc[best_model]\n",
    "    print(f\"✅ Layer {layer} best model: {best_model} | AUC={best_auc:.4f}\")\n",
    "    \n",
    "    # 如果 AUC 没提升，停止\n",
    "    if best_auc <= best_overall_auc:\n",
    "        print(\"\\n❌ AUC no longer improved. Stop stacking.\")\n",
    "        break\n",
    "    \n",
    "    # 更新最优\n",
    "    best_overall_auc = best_auc\n",
    "    best_overall_prob = layer_probs[best_model]\n",
    "    df[f'layer{layer}_best_{best_model}'] = best_overall_prob\n",
    "    \n",
    "    # 为下一层选择 topN 模型\n",
    "    topN = min(5, len(layer_auc))\n",
    "    topN_models = sorted(layer_auc.items(), key=lambda x: x[1], reverse=True)[:topN]\n",
    "    topN_probs = np.vstack([layer_probs[name] for name, _ in topN_models]).T\n",
    "    # Platt + Isotonic 校准\n",
    "    topN_probs_platt = [platt_cv(p, y) for p in topN_probs.T]\n",
    "    topN_probs_iso = [isotonic_cv(p, y) for p in topN_probs.T]\n",
    "    current_probs = np.vstack(topN_probs_platt + topN_probs_iso).T\n",
    "    \n",
    "    layer += 1\n",
    "\n",
    "# 保存最终最佳概率\n",
    "df[['case','label']].copy()\n",
    "df['final_best_ensemble'] = best_overall_prob\n",
    "df.to_csv('automated_multi_layer_ensemble.csv', index=False)\n",
    "print(f\"\\n✅ Final best ensemble saved → automated_multi_layer_ensemble.csv | AUC={best_overall_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
