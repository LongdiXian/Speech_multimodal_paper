{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3663acce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af757a35",
   "metadata": {},
   "source": [
    "# Single DLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef883ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, ComplementNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.special import expit\n",
    "import glob, os\n",
    "\n",
    "# ================= Calibration functions =================\n",
    "def platt_cv(p, y, n_splits=5):\n",
    "    out = np.zeros_like(p)\n",
    "    skf = StratifiedKFold(n_splits, shuffle=True, random_state=42)\n",
    "    for tr, te in skf.split(p, y):\n",
    "        lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        lr.fit(p[tr].reshape(-1,1), y[tr])\n",
    "        out[te] = lr.predict_proba(p[te].reshape(-1,1))[:,1]\n",
    "    return out\n",
    "\n",
    "def isotonic_cv(p, y, n_splits=5):\n",
    "    out = np.zeros_like(p)\n",
    "    skf = StratifiedKFold(n_splits, shuffle=True, random_state=42)\n",
    "    for tr, te in skf.split(p, y):\n",
    "        iso = IsotonicRegression(out_of_bounds='clip')\n",
    "        iso.fit(p[tr], y[tr])\n",
    "        out[te] = iso.transform(p[te])\n",
    "    return out\n",
    "\n",
    "# ================= Candidate stacking models =================\n",
    "candidate_models = {\n",
    "    'LR': LogisticRegression(solver='lbfgs', max_iter=2000),\n",
    "    'Ridge': RidgeClassifier(max_iter=2000),\n",
    "    'SGD': SGDClassifier(max_iter=2000, tol=1e-5),\n",
    "    'Perceptron': Perceptron(max_iter=2000),\n",
    "    'PassiveAggressive': PassiveAggressiveClassifier(max_iter=2000),\n",
    "    'RF': RandomForestClassifier(n_estimators=500, max_depth=5, random_state=6),\n",
    "    'ExtraTrees': ExtraTreesClassifier(n_estimators=500, max_depth=5, random_state=6),\n",
    "    'GBDT': GradientBoostingClassifier(n_estimators=500, learning_rate=1e-5),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=500, learning_rate=1e-5),\n",
    "    'Bagging': BaggingClassifier(n_estimators=500),\n",
    "    'XGB': XGBClassifier(n_estimators=500, learning_rate=1e-5,\n",
    "                          use_label_encoder=False, eval_metric='logloss'),\n",
    "    'SVC_rbf': SVC(probability=True, kernel='rbf'),\n",
    "    'SVC_linear': SVC(probability=True, kernel='linear'),\n",
    "    'NuSVC': NuSVC(probability=True),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'BernoulliNB': BernoulliNB(),\n",
    "    'ComplementNB': ComplementNB(),\n",
    "    'MLP_small': MLPClassifier(hidden_layer_sizes=(32,16), max_iter=2000, random_state=6),\n",
    "    'MLP_large': MLPClassifier(hidden_layer_sizes=(64,32), max_iter=2000, random_state=6)\n",
    "}\n",
    "\n",
    "# ================= Layer-wise Boosting models =================\n",
    "layer_boosters = {\n",
    "    'Layer_GBDT': GradientBoostingClassifier(\n",
    "        n_estimators=300, learning_rate=0.05, max_depth=3),\n",
    "    'Layer_XGB': XGBClassifier(\n",
    "        n_estimators=300, learning_rate=0.05, max_depth=3,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        eval_metric='logloss', use_label_encoder=False)\n",
    "}\n",
    "\n",
    "# ================= Multi-layer stacking + Boosting =================\n",
    "def multi_layer_stacking(df, prob_cols, label_col='label',\n",
    "                          max_layers=10, topN=5):\n",
    "\n",
    "    y = df[label_col].values\n",
    "    base_probs = [df[c].values for c in prob_cols]\n",
    "\n",
    "    # Initial calibration\n",
    "    probs_platt = [platt_cv(p, y) for p in base_probs]\n",
    "    probs_iso = [isotonic_cv(p, y) for p in base_probs]\n",
    "    current_probs = np.vstack(probs_platt + probs_iso).T\n",
    "\n",
    "    skf = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "    best_overall_auc = 0\n",
    "    best_overall_prob = None\n",
    "    layer = 1\n",
    "\n",
    "    while layer <= max_layers:\n",
    "        print(f\"\\n=== Layer {layer} ===\")\n",
    "        layer_probs = {}\n",
    "        layer_auc = {}\n",
    "\n",
    "        # ---------- normal stacking models ----------\n",
    "        for name, model in candidate_models.items():\n",
    "            stack_prob = np.zeros_like(y, dtype=float)\n",
    "\n",
    "            for tr, te in skf.split(current_probs, y):\n",
    "                model.fit(current_probs[tr], y[tr])\n",
    "                if hasattr(model, \"predict_proba\"):\n",
    "                    stack_prob[te] = model.predict_proba(current_probs[te])[:,1]\n",
    "                elif hasattr(model, \"decision_function\"):\n",
    "                    stack_prob[te] = expit(model.decision_function(current_probs[te]))\n",
    "                else:\n",
    "                    stack_prob[te] = model.predict(current_probs[te])\n",
    "\n",
    "            orig_prob = current_probs.mean(axis=1)\n",
    "            ensemble_prob = 0.5 * stack_prob + 0.5 * orig_prob\n",
    "\n",
    "            layer_probs[name] = ensemble_prob\n",
    "            layer_auc[name] = roc_auc_score(y, ensemble_prob)\n",
    "            print(f\"{name}: AUC={layer_auc[name]:.4f}\")\n",
    "\n",
    "        # ---------- layer-wise boosting ----------\n",
    "        for bname, booster in layer_boosters.items():\n",
    "            boost_prob = np.zeros_like(y, dtype=float)\n",
    "\n",
    "            for tr, te in skf.split(current_probs, y):\n",
    "                booster.fit(current_probs[tr], y[tr])\n",
    "                boost_prob[te] = booster.predict_proba(current_probs[te])[:,1]\n",
    "\n",
    "            orig_prob = current_probs.mean(axis=1)\n",
    "            ensemble_boost = 0.5 * boost_prob + 0.5 * orig_prob\n",
    "\n",
    "            layer_probs[bname] = ensemble_boost\n",
    "            layer_auc[bname] = roc_auc_score(y, ensemble_boost)\n",
    "            print(f\"{bname}: AUC={layer_auc[bname]:.4f}\")\n",
    "\n",
    "        # ---------- select best ----------\n",
    "        best_model = max(layer_auc, key=lambda k: layer_auc[k])\n",
    "        best_auc = layer_auc[best_model]\n",
    "        print(f\"✅ Layer {layer} best: {best_model} | AUC={best_auc:.4f}\")\n",
    "\n",
    "        if best_auc <= best_overall_auc:\n",
    "            print(\"❌ AUC not improved. Stop.\")\n",
    "            break\n",
    "\n",
    "        best_overall_auc = best_auc\n",
    "        best_overall_prob = layer_probs[best_model]\n",
    "        df[f'layer{layer}_best_{best_model}'] = best_overall_prob\n",
    "\n",
    "        # ---------- top-N enter next layer ----------\n",
    "        topN_models = sorted(layer_auc.items(),\n",
    "                             key=lambda x: x[1], reverse=True)[:topN]\n",
    "        topN_probs = np.vstack(\n",
    "            [layer_probs[name] for name, _ in topN_models]).T\n",
    "\n",
    "        topN_platt = [platt_cv(p, y) for p in topN_probs.T]\n",
    "        topN_iso = [isotonic_cv(p, y) for p in topN_probs.T]\n",
    "        current_probs = np.vstack(topN_platt + topN_iso).T\n",
    "\n",
    "        layer += 1\n",
    "\n",
    "    df['final_best_ensemble'] = best_overall_prob\n",
    "    return df, best_overall_auc\n",
    "\n",
    "# ================= Load datasets =================\n",
    "\n",
    "# data1\n",
    "data1 = pd.read_csv('OPENSMILE_probabilities.csv')\n",
    "prob_cols1 = [c for c in data1.columns if c not in ['case','label']]\n",
    "df1, auc1 = multi_layer_stacking(data1, prob_cols1)\n",
    "df1.to_csv('data1_multi_layer_ensemble_boosting.csv', index=False)\n",
    "\n",
    "# data2\n",
    "data2 = pd.read_csv('./result/Normal single models results/Normal_seven_results.csv')\n",
    "prob_cols2 = [c for c in data2.columns if c not in ['case','label']]\n",
    "df2, auc2 = multi_layer_stacking(data2, prob_cols2)\n",
    "df2.to_csv('data2_multi_layer_ensemble_boosting.csv', index=False)\n",
    "\n",
    "# data3 (multiple CSV)\n",
    "data3_folder = './result/text_model/'\n",
    "files = glob.glob(os.path.join(data3_folder, '*.csv'))\n",
    "\n",
    "dfs = []\n",
    "for i, f in enumerate(files):\n",
    "    tmp = pd.read_csv(f)\n",
    "    dfs.append(tmp[['prob']].rename(columns={'prob': f'prob{i}'}))\n",
    "\n",
    "labels = pd.read_csv(files[0])['label'].values\n",
    "df3 = pd.concat(dfs, axis=1)\n",
    "df3['label'] = labels\n",
    "\n",
    "df3, auc3 = multi_layer_stacking(df3, [c for c in df3.columns if c != 'label'])\n",
    "df3.to_csv('data3_multi_layer_ensemble_boosting.csv', index=False)\n",
    "\n",
    "# data4\n",
    "data4 = pd.read_csv('lingustic_probabilities.csv')\n",
    "prob_cols4 = [c for c in data4.columns if c not in ['case','label']]\n",
    "df4, auc4 = multi_layer_stacking(data4, prob_cols4)\n",
    "df4.to_csv('data4_multi_layer_ensemble_boosting.csv', index=False)\n",
    "\n",
    "print(f\"\\n✅ Final AUCs:\")\n",
    "print(f\"data1={auc1:.4f}, data2={auc2:.4f}, data3={auc3:.4f}, data4={auc4:.4f}\")\n",
    "print(\"✅ All boosting multi-layer ensemble results saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9d813",
   "metadata": {},
   "source": [
    "# multi DLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb45e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, precision_score, recall_score,\n",
    "    accuracy_score, confusion_matrix\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# ===================== 读取多层堆叠输出数据 =====================\n",
    "data1 = pd.read_csv('data1_multi_layer_ensemble.csv')\n",
    "data2 = pd.read_csv('data2_multi_layer_ensemble.csv')\n",
    "data3 = pd.read_csv('data3_multi_layer_ensemble.csv')\n",
    "data4 = pd.read_csv('data4_multi_layer_ensemble.csv')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'case': data1['case'],\n",
    "    'prob1': data1['final_best_ensemble'],\n",
    "    'label': data1['label'],\n",
    "    'prob2': data2['final_best_ensemble'],\n",
    "    'prob3': data3['final_best_ensemble'],\n",
    "    'prob4': data4['final_best_ensemble']\n",
    "})\n",
    "\n",
    "# ===================== 评价指标函数 =====================\n",
    "def calculate_metrics(y_true, y_pred_probs, threshold=0.5):\n",
    "    y_pred = (y_pred_probs >= threshold).astype(int)\n",
    "    auc = roc_auc_score(y_true, y_pred_probs)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    ppv = precision\n",
    "    npv = cm[0, 0] / (cm[0, 0] + cm[1, 0]) if (cm[0, 0] + cm[1, 0]) > 0 else 0\n",
    "    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1]) if (cm[0, 0] + cm[0, 1]) > 0 else 0\n",
    "    positives = y_pred_probs[y_true == 1]\n",
    "    negatives = y_pred_probs[y_true == 0]\n",
    "    auc_pvalue = mannwhitneyu(positives, negatives, alternative='two-sided').pvalue\n",
    "    return {\n",
    "        \"AUC\": auc,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"AUC P-Value\": auc_pvalue,\n",
    "        \"PPV\": ppv,\n",
    "        \"NPV\": npv,\n",
    "        \"Sensitivity\": recall,\n",
    "        \"Specificity\": specificity\n",
    "    }\n",
    "\n",
    "def find_best_f1_threshold(y_true, probs):\n",
    "    best_f1 = 0\n",
    "    best_thresh = 0.5\n",
    "    for thresh in np.arange(0.1, 0.9, 0.01):\n",
    "        y_pred = (probs >= thresh).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thresh = thresh\n",
    "    return best_thresh\n",
    "\n",
    "# ===================== 1. 简单平均集成 =====================\n",
    "df['ensemble_prob_mean'] = df[['prob1', 'prob2', 'prob3', 'prob4']].mean(axis=1)\n",
    "mean_thresh = find_best_f1_threshold(df['label'], df['ensemble_prob_mean'])\n",
    "metrics_mean = calculate_metrics(df['label'], df['ensemble_prob_mean'], threshold=mean_thresh)\n",
    "print('\\n【1. 简单平均集成】')\n",
    "print(f'最佳F1阈值={mean_thresh:.2f}')\n",
    "for k, v in metrics_mean.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# ===================== 2. 自动加权平均（最优权重） =====================\n",
    "def weighted_auc(weights):\n",
    "    weights = np.clip(weights, 0, 1)\n",
    "    weights = weights / weights.sum()\n",
    "    ensemble_probs = (\n",
    "        weights[0] * df['prob1'] +\n",
    "        weights[1] * df['prob2'] +\n",
    "        weights[2] * df['prob3'] +\n",
    "        weights[3] * df['prob4']\n",
    "    )\n",
    "    return -roc_auc_score(df['label'], ensemble_probs)\n",
    "\n",
    "init_weights = [0.25, 0.25, 0.25, 0.25]\n",
    "cons = {'type': 'eq', 'fun': lambda w: w.sum() - 1}\n",
    "bounds = [(0, 1)] * 4\n",
    "\n",
    "res = minimize(weighted_auc, init_weights, bounds=bounds, constraints=cons)\n",
    "opt_weights = res.x / res.x.sum()\n",
    "df['ensemble_prob_weighted'] = (\n",
    "    opt_weights[0] * df['prob1'] +\n",
    "    opt_weights[1] * df['prob2'] +\n",
    "    opt_weights[2] * df['prob3'] +\n",
    "    opt_weights[3] * df['prob4']\n",
    ")\n",
    "weighted_thresh = find_best_f1_threshold(df['label'], df['ensemble_prob_weighted'])\n",
    "metrics_weighted = calculate_metrics(df['label'], df['ensemble_prob_weighted'], threshold=weighted_thresh)\n",
    "print('\\n【2. 自动加权集成】')\n",
    "print('最优权重:', [f\"{w:.3f}\" for w in opt_weights])\n",
    "print(f'最佳F1阈值={weighted_thresh:.2f}')\n",
    "for k, v in metrics_weighted.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# ===================== 3. Stacking集成（逻辑回归元学习器，交叉验证） =====================\n",
    "X = df[['prob1', 'prob2', 'prob3', 'prob4']].values\n",
    "y = df['label'].values\n",
    "\n",
    "meta_probs = np.zeros_like(y, dtype=float)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_idx, test_idx in skf.split(X, y):\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X[train_idx], y[train_idx])\n",
    "    meta_probs[test_idx] = clf.predict_proba(X[test_idx])[:, 1]\n",
    "\n",
    "df['ensemble_prob_stacking'] = meta_probs\n",
    "stacking_thresh = find_best_f1_threshold(y, df['ensemble_prob_stacking'])\n",
    "metrics_stacking = calculate_metrics(y, df['ensemble_prob_stacking'], threshold=stacking_thresh)\n",
    "print('\\n【3. Stacking集成（逻辑回归）】')\n",
    "print(f'最佳F1阈值={stacking_thresh:.2f}')\n",
    "for k, v in metrics_stacking.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# ===================== 4. 多层集成（再次stacking，融合单模型与前三种集成结果） =====================\n",
    "feature_cols_multi = [\n",
    "    'prob1', 'prob2', 'prob3', 'prob4',\n",
    "    'ensemble_prob_stacking', 'ensemble_prob_mean', 'ensemble_prob_weighted'\n",
    "]\n",
    "X_multi = df[feature_cols_multi].values\n",
    "y_multi = df['label'].values\n",
    "\n",
    "meta_probs_multi = np.zeros_like(y_multi, dtype=float)\n",
    "skf_multi = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_idx, test_idx in skf_multi.split(X_multi, y_multi):\n",
    "    clf_multi = LogisticRegression(max_iter=1000)\n",
    "    clf_multi.fit(X_multi[train_idx], y_multi[train_idx])\n",
    "    meta_probs_multi[test_idx] = clf_multi.predict_proba(X_multi[test_idx])[:, 1]\n",
    "\n",
    "df['ensemble_prob_final'] = meta_probs_multi\n",
    "final_thresh = find_best_f1_threshold(y_multi, df['ensemble_prob_final'])\n",
    "metrics_final = calculate_metrics(y_multi, df['ensemble_prob_final'], threshold=final_thresh)\n",
    "print('\\n【4. 多层集成：最终stacking】')\n",
    "print(f'最佳F1阈值={final_thresh:.2f}')\n",
    "for k, v in metrics_final.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# ===================== 自动化选择最佳集成方法 =====================\n",
    "results = {\n",
    "    'stacking': {\n",
    "        'name': 'Stacking集成（逻辑回归）',\n",
    "        'metrics': metrics_stacking,\n",
    "        'probs': df['ensemble_prob_stacking'],\n",
    "        'threshold': stacking_thresh\n",
    "    },\n",
    "    'mean': {\n",
    "        'name': '简单平均集成',\n",
    "        'metrics': metrics_mean,\n",
    "        'probs': df['ensemble_prob_mean'],\n",
    "        'threshold': mean_thresh\n",
    "    },\n",
    "    'weighted': {\n",
    "        'name': '自动加权集成',\n",
    "        'metrics': metrics_weighted,\n",
    "        'probs': df['ensemble_prob_weighted'],\n",
    "        'threshold': weighted_thresh\n",
    "    },\n",
    "    'final': {\n",
    "        'name': '多层Stacking（最终）',\n",
    "        'metrics': metrics_final,\n",
    "        'probs': df['ensemble_prob_final'],\n",
    "        'threshold': final_thresh\n",
    "    }\n",
    "}\n",
    "\n",
    "key_metric = 'AUC'  # 可以改成 F1-Score 等\n",
    "best_key = max(results, key=lambda k: results[k]['metrics'][key_metric])\n",
    "best_method = results[best_key]\n",
    "\n",
    "print('\\n【自动化选择最佳集成方法】')\n",
    "print(f\"最佳方法: {best_method['name']}\")\n",
    "print(f\"{key_metric}: {best_method['metrics'][key_metric]:.4f}\")\n",
    "print(f\"全部指标:\")\n",
    "for k, v in best_method['metrics'].items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "print(f\"最佳阈值（用于分类）: {best_method['threshold']:.2f}\")\n",
    "\n",
    "# 输出最佳预测标签\n",
    "y_pred_best = (best_method['probs'] >= best_method['threshold']).astype(int)\n",
    "df['y_pred_best'] = y_pred_best\n",
    "\n",
    "# 计算最优概率与四单模型概率的P-Value（Mann-Whitney U检验）\n",
    "best_prob = best_method['probs']\n",
    "p_values = {}\n",
    "for col in ['prob1', 'prob2', 'prob3', 'prob4']:\n",
    "    stat, pval = mannwhitneyu(best_prob, df[col], alternative='two-sided')\n",
    "    p_values[f'ensemble_vs_{col}'] = pval\n",
    "\n",
    "print(\"\\n【最优集成概率与各单模型概率的P-Value（Mann-Whitney U 检验）】\")\n",
    "for k, v in p_values.items():\n",
    "    print(f\"{k}: {v:.6f}\")\n",
    "\n",
    "# 如需保存结果，取消注释以下行\n",
    "df.to_csv('final_ensemble_result_singleensemble.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
